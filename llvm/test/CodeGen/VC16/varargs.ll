; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=vc16 -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=VC16I %s

declare void @llvm.va_start(i8*)
declare void @llvm.va_end(i8*)

declare void @notdead(i8*)

; Although frontends are recommended to not generate va_arg due to the lack of
; support for aggregate types, we test simple cases here to ensure they are
; lowered correctly

define i16 @va1(i8* %fmt, ...) nounwind {
; VC16I-LABEL: va1:
; VC16I:       ; %bb.0:
; VC16I-NEXT:    addi sp, -12
; VC16I-NEXT:    sw s1, 6(sp)
; VC16I-NEXT:    sw ra, 4(sp)
; VC16I-NEXT:    mv s1, sp
; VC16I-NEXT:    addi s1, 8
; VC16I-NEXT:    mv a0, a1
; VC16I-NEXT:    sw a2, 10(sp)
; VC16I-NEXT:    lli a1, 12
; VC16I-NEXT:    add a1, sp
; VC16I-NEXT:    sw a1, 0(sp)
; VC16I-NEXT:    sw a0, 8(sp)
; VC16I-NEXT:    mv sp, s1
; VC16I-NEXT:    addi sp, -8
; VC16I-NEXT:    lw ra, 4(sp)
; VC16I-NEXT:    lw s1, 6(sp)
; VC16I-NEXT:    addi sp, 12
; VC16I-NEXT:    jalr t0, ra, 0
  %va = alloca i8*, align 4
  %1 = bitcast i8** %va to i8*
  call void @llvm.va_start(i8* %1)
  %argp.cur = load i8*, i8** %va, align 4
  %argp.next = getelementptr inbounds i8, i8* %argp.cur, i16 4
  store i8* %argp.next, i8** %va, align 4
  %2 = bitcast i8* %argp.cur to i16*
  %3 = load i16, i16* %2, align 4
  call void @llvm.va_end(i8* %1)
  ret i16 %3
}


define i16 @va1_va_arg(i8* %fmt, ...) nounwind {
; VC16I-LABEL: va1_va_arg:
; VC16I:       ; %bb.0:
; VC16I-NEXT:    addi sp, -12
; VC16I-NEXT:    sw s1, 6(sp)
; VC16I-NEXT:    sw ra, 4(sp)
; VC16I-NEXT:    mv s1, sp
; VC16I-NEXT:    addi s1, 8
; VC16I-NEXT:    mv a0, a1
; VC16I-NEXT:    sw a2, 10(sp)
; VC16I-NEXT:    lli a1, 10
; VC16I-NEXT:    add a1, sp
; VC16I-NEXT:    sw a1, 0(sp)
; VC16I-NEXT:    sw a0, 8(sp)
; VC16I-NEXT:    mv sp, s1
; VC16I-NEXT:    addi sp, -8
; VC16I-NEXT:    lw ra, 4(sp)
; VC16I-NEXT:    lw s1, 6(sp)
; VC16I-NEXT:    addi sp, 12
; VC16I-NEXT:    jalr t0, ra, 0
  %va = alloca i8*, align 4
  %1 = bitcast i8** %va to i8*
  call void @llvm.va_start(i8* %1)
  %2 = va_arg i8** %va, i16
  call void @llvm.va_end(i8* %1)
  ret i16 %2
}

; Ensure the adjustment when restoring the stack pointer using the frame
; pointer is correct
define i16 @va1_va_arg_alloca(i8* %fmt, ...) nounwind {
; VC16I-LABEL: va1_va_arg_alloca:
; VC16I:       ; %bb.0:
; VC16I-NEXT:    addi sp, -12
; VC16I-NEXT:    sw s0, 6(sp)
; VC16I-NEXT:    sw s1, 4(sp)
; VC16I-NEXT:    sw ra, 2(sp)
; VC16I-NEXT:    mv s1, sp
; VC16I-NEXT:    addi s1, 8
; VC16I-NEXT:    mv s0, a1
; VC16I-NEXT:    sw a2, 10(sp)
; VC16I-NEXT:    lli a0, 10
; VC16I-NEXT:    add a0, sp
; VC16I-NEXT:    sw a0, 0(sp)
; VC16I-NEXT:    sw s0, 8(sp)
; VC16I-NEXT:    addi a1, 1
; VC16I-NEXT:    andi a1, -2
; VC16I-NEXT:    mv a0, sp
; VC16I-NEXT:    sub a0, a1
; VC16I-NEXT:    mv sp, a0
; VC16I-NEXT:    lui a1, %his(notdead)
; VC16I-NEXT:    addi a1, %lo(notdead)
; VC16I-NEXT:    jalr ra, a1, 0
; VC16I-NEXT:    mv a0, s0
; VC16I-NEXT:    mv sp, s1
; VC16I-NEXT:    addi sp, -8
; VC16I-NEXT:    lw ra, 2(sp)
; VC16I-NEXT:    lw s1, 4(sp)
; VC16I-NEXT:    lw s0, 6(sp)
; VC16I-NEXT:    addi sp, 12
; VC16I-NEXT:    jalr t0, ra, 0
  %va = alloca i8*, align 4
  %1 = bitcast i8** %va to i8*
  call void @llvm.va_start(i8* %1)
  %2 = va_arg i8** %va, i16
  %3 = alloca i8, i16 %2
  call void @notdead(i8* %3)
  call void @llvm.va_end(i8* %1)
  ret i16 %2
}

define void @va1_caller() nounwind {
; VC16I-LABEL: va1_caller:
; VC16I:       ; %bb.0:
; VC16I-NEXT:    addi sp, -12
; VC16I-NEXT:    sw s1, 10(sp)
; VC16I-NEXT:    sw ra, 8(sp)
; VC16I-NEXT:    mv s1, sp
; VC16I-NEXT:    addi s1, 12
; VC16I-NEXT:    lli a0, 0
; VC16I-NEXT:    sw a0, 4(sp)
; VC16I-NEXT:    sw a0, 2(sp)
; VC16I-NEXT:    sw a0, 0(sp)
; VC16I-NEXT:    lui a0, 512
; VC16I-NEXT:    addi a0, -16
; VC16I-NEXT:    sw a0, 6(sp)
; VC16I-NEXT:    lui a0, %his(va1)
; VC16I-NEXT:    addi a0, %lo(va1)
; VC16I-NEXT:    mv a1, sp
; VC16I-NEXT:    lli a2, 2
; VC16I-NEXT:    jalr ra, a0, 0
; VC16I-NEXT:    lw ra, 8(sp)
; VC16I-NEXT:    lw s1, 10(sp)
; VC16I-NEXT:    addi sp, 12
; VC16I-NEXT:    jalr t0, ra, 0
; Pass a double, as a float would be promoted by a C/C++ frontend
  %1 = call i16 (i8*, ...) @va1(i8* undef, double 1.0, i16 2)
  ret void
}


; Ensure a named float argument is passed in a1 and a2, while the vararg
; float is passed via stack
define float @va3(i16 %a, float %b, ...) nounwind {
; VC16I-LABEL: va3:
; VC16I:       ; %bb.0:
; VC16I-NEXT:    addi sp, -12
; VC16I-NEXT:    sw s0, 10(sp)
; VC16I-NEXT:    sw s1, 8(sp)
; VC16I-NEXT:    sw ra, 6(sp)
; VC16I-NEXT:    mv s1, sp
; VC16I-NEXT:    addi s1, 12
; VC16I-NEXT:    mv t0, a2
; VC16I-NEXT:    mv a0, a1
; VC16I-NEXT:    lui a1, 1
; VC16I-NEXT:    addi a1, -5
; VC16I-NEXT:    add a1, sp
; VC16I-NEXT:    sw a1, 4(sp)
; VC16I-NEXT:    lui a1, 1
; VC16I-NEXT:    addi a1, -13
; VC16I-NEXT:    add a1, sp
; VC16I-NEXT:    andi a1, -8
; VC16I-NEXT:    mv a2, a1
; VC16I-NEXT:    ori a2, 2
; VC16I-NEXT:    lw a2, 0(a2)
; VC16I-NEXT:    sw a2, 0(sp)
; VC16I-NEXT:    lui s0, %his(__addsf3)
; VC16I-NEXT:    addi s0, %lo(__addsf3)
; VC16I-NEXT:    lw a2, 0(a1)
; VC16I-NEXT:    mv a1, t0
; VC16I-NEXT:    jalr ra, s0, 0
; VC16I-NEXT:    mv sp, s1
; VC16I-NEXT:    addi sp, -12
; VC16I-NEXT:    lw ra, 6(sp)
; VC16I-NEXT:    lw s1, 8(sp)
; VC16I-NEXT:    lw s0, 10(sp)
; VC16I-NEXT:    addi sp, 12
; VC16I-NEXT:    jalr t0, ra, 0
  %va = alloca i8*, align 4
  %1 = bitcast i8** %va to i8*
  call void @llvm.va_start(i8* %1)
  %2 = bitcast i8** %va to i16*
  %argp.cur = load i16, i16* %2, align 4
  %3 = add i16 %argp.cur, 7
  %4 = and i16 %3, -8
  %argp.cur.aligned = inttoptr i16 %3 to i8*
  %argp.next = getelementptr inbounds i8, i8* %argp.cur.aligned, i16 8
  store i8* %argp.next, i8** %va, align 4
  %5 = inttoptr i16 %4 to float*
  %6 = load float, float* %5, align 8
  call void @llvm.va_end(i8* %1)
  %7 = fadd float %b, %6
  ret float %7
}

define float @va3_va_arg(i16 %a, float %b, ...) nounwind {
; VC16I-LABEL: va3_va_arg:
; VC16I:       ; %bb.0:
; VC16I-NEXT:    addi sp, -12
; VC16I-NEXT:    sw s0, 10(sp)
; VC16I-NEXT:    sw s1, 8(sp)
; VC16I-NEXT:    sw ra, 6(sp)
; VC16I-NEXT:    mv s1, sp
; VC16I-NEXT:    addi s1, 12
; VC16I-NEXT:    mv t0, a2
; VC16I-NEXT:    mv a0, a1
; VC16I-NEXT:    lw a1, 14(sp)
; VC16I-NEXT:    sw a1, 0(sp)
; VC16I-NEXT:    lui a1, 1
; VC16I-NEXT:    addi a1, -16
; VC16I-NEXT:    add a1, sp
; VC16I-NEXT:    sw a1, 4(sp)
; VC16I-NEXT:    lui s0, %his(__addsf3)
; VC16I-NEXT:    addi s0, %lo(__addsf3)
; VC16I-NEXT:    lw a2, 12(sp)
; VC16I-NEXT:    mv a1, t0
; VC16I-NEXT:    jalr ra, s0, 0
; VC16I-NEXT:    mv sp, s1
; VC16I-NEXT:    addi sp, -12
; VC16I-NEXT:    lw ra, 6(sp)
; VC16I-NEXT:    lw s1, 8(sp)
; VC16I-NEXT:    lw s0, 10(sp)
; VC16I-NEXT:    addi sp, 12
; VC16I-NEXT:    jalr t0, ra, 0
  %va = alloca i8*, align 4
  %1 = bitcast i8** %va to i8*
  call void @llvm.va_start(i8* %1)
  %2 = va_arg i8** %va, float
  call void @llvm.va_end(i8* %1)
  %3 = fadd float %b, %2
  ret float %3
}

define void @va3_caller() nounwind {
; VC16I-LABEL: va3_caller:
; VC16I:       ; %bb.0:
; VC16I-NEXT:    addi sp, -8
; VC16I-NEXT:    sw s1, 6(sp)
; VC16I-NEXT:    sw ra, 4(sp)
; VC16I-NEXT:    mv s1, sp
; VC16I-NEXT:    addi s1, 8
; VC16I-NEXT:    lui a0, 512
; VC16I-NEXT:    sw a0, 2(sp)
; VC16I-NEXT:    lli a1, 0
; VC16I-NEXT:    sw a1, 0(sp)
; VC16I-NEXT:    lui t0, %his(va3)
; VC16I-NEXT:    addi t0, %lo(va3)
; VC16I-NEXT:    lli a0, 2
; VC16I-NEXT:    lui a2, 508
; VC16I-NEXT:    jalr ra, t0, 0
; VC16I-NEXT:    lw ra, 4(sp)
; VC16I-NEXT:    lw s1, 6(sp)
; VC16I-NEXT:    addi sp, 8
; VC16I-NEXT:    jalr t0, ra, 0
 %1 = call float (i16, float, ...) @va3(i16 2, float 1.000000e+00, float 2.000000e+00)
 ret void
}

declare void @llvm.va_copy(i8*, i8*)

define i16 @va4_va_copy(i16 %argno, ...) nounwind {
; VC16I-LABEL: va4_va_copy:
; VC16I:       ; %bb.0:
; VC16I-NEXT:    addi sp, -16
; VC16I-NEXT:    sw s0, 10(sp)
; VC16I-NEXT:    sw s1, 8(sp)
; VC16I-NEXT:    sw ra, 6(sp)
; VC16I-NEXT:    mv s1, sp
; VC16I-NEXT:    addi s1, 12
; VC16I-NEXT:    mv s0, a1
; VC16I-NEXT:    sw a2, 14(sp)
; VC16I-NEXT:    sw s0, 12(sp)
; VC16I-NEXT:    lli a0, 14
; VC16I-NEXT:    add a0, sp
; VC16I-NEXT:    sw a0, 4(sp)
; VC16I-NEXT:    sw a0, 0(sp)
; VC16I-NEXT:    lui a1, %his(notdead)
; VC16I-NEXT:    addi a1, %lo(notdead)
; VC16I-NEXT:    jalr ra, a1, 0
; VC16I-NEXT:    lw a1, 4(sp)
; VC16I-NEXT:    addi a1, 1
; VC16I-NEXT:    andi a1, -2
; VC16I-NEXT:    mv a0, a1
; VC16I-NEXT:    addi a0, 2
; VC16I-NEXT:    sw a0, 4(sp)
; VC16I-NEXT:    lw a0, 0(a1)
; VC16I-NEXT:    addi a1, 3
; VC16I-NEXT:    andi a1, -2
; VC16I-NEXT:    mv a2, a1
; VC16I-NEXT:    addi a2, 2
; VC16I-NEXT:    sw a2, 4(sp)
; VC16I-NEXT:    lw a2, 0(a1)
; VC16I-NEXT:    addi a1, 3
; VC16I-NEXT:    andi a1, -2
; VC16I-NEXT:    mv t0, a1
; VC16I-NEXT:    addi t0, 2
; VC16I-NEXT:    sw t0, 4(sp)
; VC16I-NEXT:    add a0, s0
; VC16I-NEXT:    add a0, a2
; VC16I-NEXT:    lw a1, 0(a1)
; VC16I-NEXT:    add a0, a1
; VC16I-NEXT:    mv sp, s1
; VC16I-NEXT:    addi sp, -12
; VC16I-NEXT:    lw ra, 6(sp)
; VC16I-NEXT:    lw s1, 8(sp)
; VC16I-NEXT:    lw s0, 10(sp)
; VC16I-NEXT:    addi sp, 15
; VC16I-NEXT:    addi sp, 1
; VC16I-NEXT:    jalr t0, ra, 0
  %vargs = alloca i8*, align 4
  %wargs = alloca i8*, align 4
  %1 = bitcast i8** %vargs to i8*
  %2 = bitcast i8** %wargs to i8*
  call void @llvm.va_start(i8* %1)
  %3 = va_arg i8** %vargs, i16
  call void @llvm.va_copy(i8* %2, i8* %1)
  %4 = load i8*, i8** %wargs, align 4
  call void @notdead(i8* %4)
  %5 = va_arg i8** %vargs, i16
  %6 = va_arg i8** %vargs, i16
  %7 = va_arg i8** %vargs, i16
  call void @llvm.va_end(i8* %1)
  call void @llvm.va_end(i8* %2)
  %add1 = add i16 %5, %3
  %add2 = add i16 %add1, %6
  %add3 = add i16 %add2, %7
  ret i16 %add3
}

; Check 2x*xlen values are aligned appropriately when passed on the stack in a vararg call

define i16 @va5_aligned_stack_callee(i16 %a, ...) nounwind {
; VC16I-LABEL: va5_aligned_stack_callee:
; VC16I:       ; %bb.0:
; VC16I-NEXT:    addi sp, -8
; VC16I-NEXT:    sw s1, 2(sp)
; VC16I-NEXT:    sw ra, 0(sp)
; VC16I-NEXT:    mv s1, sp
; VC16I-NEXT:    addi s1, 4
; VC16I-NEXT:    sw a2, 6(sp)
; VC16I-NEXT:    sw a1, 4(sp)
; VC16I-NEXT:    lli a0, 1
; VC16I-NEXT:    lw ra, 0(sp)
; VC16I-NEXT:    lw s1, 2(sp)
; VC16I-NEXT:    addi sp, 8
; VC16I-NEXT:    jalr t0, ra, 0
  ret i16 1
}

define void @va5_aligned_stack_caller() nounwind {
; The float should be 4-byte aligned on the stack, but the two-element array
; should only be 2-byte aligned
; VC16I-LABEL: va5_aligned_stack_caller:
; VC16I:       ; %bb.0:
; VC16I-NEXT:    lui a0, 3
; VC16I-NEXT:    addi a0, -16
; VC16I-NEXT:    sub sp, a0
; VC16I-NEXT:    lui a0, 2
; VC16I-NEXT:    add a0, sp
; VC16I-NEXT:    sw s1, 14(a0)
; VC16I-NEXT:    lui a0, 2
; VC16I-NEXT:    add a0, sp
; VC16I-NEXT:    sw ra, 12(a0)
; VC16I-NEXT:    mv s1, sp
; VC16I-NEXT:    lui a0, 3
; VC16I-NEXT:    addi a0, -16
; VC16I-NEXT:    add s1, a0
; VC16I-NEXT:    lui a0, 1
; VC16I-NEXT:    mv a1, a0
; VC16I-NEXT:    addi a1, -15
; VC16I-NEXT:    sw a1, 16(sp)
; VC16I-NEXT:    addi a0, -16
; VC16I-NEXT:    sw a0, 14(sp)
; VC16I-NEXT:    lli a0, 15
; VC16I-NEXT:    sw a0, 12(sp)
; VC16I-NEXT:    lli a0, 14
; VC16I-NEXT:    sw a0, 6(sp)
; VC16I-NEXT:    lui a0, 1
; VC16I-NEXT:    addi a0, 8
; VC16I-NEXT:    add a0, sp
; VC16I-NEXT:    sw a0, 4(sp)
; VC16I-NEXT:    lli a0, 13
; VC16I-NEXT:    sw a0, 2(sp)
; VC16I-NEXT:    lli a0, 12
; VC16I-NEXT:    sw a0, 0(sp)
; VC16I-NEXT:    lui a0, 512
; VC16I-NEXT:    sw a0, 10(sp)
; VC16I-NEXT:    lli a1, 0
; VC16I-NEXT:    sw a1, 8(sp)
; VC16I-NEXT:    sw a1, 46(sp)
; VC16I-NEXT:    lli a1, 4
; VC16I-NEXT:    sw a1, 44(sp)
; VC16I-NEXT:    lui a1, 1600
; VC16I-NEXT:    sw a1, 40(sp)
; VC16I-NEXT:    sw a0, 62(sp)
; VC16I-NEXT:    lui a0, 1345
; VC16I-NEXT:    addi a0, -9
; VC16I-NEXT:    sw a0, 42(sp)
; VC16I-NEXT:    lui a0, 1167
; VC16I-NEXT:    addi a0, 11
; VC16I-NEXT:    sw a0, 60(sp)
; VC16I-NEXT:    lui a0, 1065
; VC16I-NEXT:    mv a1, a0
; VC16I-NEXT:    addi a1, -2
; VC16I-NEXT:    sw a1, 58(sp)
; VC16I-NEXT:    lui a1, 1475
; VC16I-NEXT:    addi a1, -15
; VC16I-NEXT:    sw a1, 56(sp)
; VC16I-NEXT:    lui a1, 1884
; VC16I-NEXT:    addi a1, 5
; VC16I-NEXT:    sw a1, 54(sp)
; VC16I-NEXT:    lui a1, 246
; VC16I-NEXT:    addi a1, -8
; VC16I-NEXT:    sw a1, 52(sp)
; VC16I-NEXT:    lui a1, 655
; VC16I-NEXT:    addi a1, 11
; VC16I-NEXT:    sw a1, 50(sp)
; VC16I-NEXT:    addi a0, -1
; VC16I-NEXT:    sw a0, 48(sp)
; VC16I-NEXT:    lui t0, %his(va5_aligned_stack_callee)
; VC16I-NEXT:    addi t0, %lo(va5_aligned_stack_callee)
; VC16I-NEXT:    lli a0, 1
; VC16I-NEXT:    lli a1, 11
; VC16I-NEXT:    lui a2, 2
; VC16I-NEXT:    addi a2, -16
; VC16I-NEXT:    add a2, sp
; VC16I-NEXT:    jalr ra, t0, 0
; VC16I-NEXT:    mv sp, s1
; VC16I-NEXT:    lui a0, 3
; VC16I-NEXT:    addi a0, -16
; VC16I-NEXT:    sub sp, a0
; VC16I-NEXT:    lui a0, 2
; VC16I-NEXT:    add a0, sp
; VC16I-NEXT:    lw ra, 12(a0)
; VC16I-NEXT:    lui a0, 2
; VC16I-NEXT:    add a0, sp
; VC16I-NEXT:    lw s1, 14(a0)
; VC16I-NEXT:    lui a0, 3
; VC16I-NEXT:    addi a0, -16
; VC16I-NEXT:    add sp, a0
; VC16I-NEXT:    jalr t0, ra, 0
  %1 = call i16 (i16, ...) @va5_aligned_stack_callee(i16 1, i16 11,
    fp128 0xLEB851EB851EB851F400091EB851EB851, i16 12, i16 13, i64 20000000000,
    i16 14, float 2.0, i16 15, [2 x i16] [i16 16, i16 17])
  ret void
}

; A function with no fixed arguments is not valid C, but can be
; specified in LLVM IR. We must ensure the vararg save area is
; still set up correctly.

define i16 @va6_no_fixed_args(...) nounwind {
; VC16I-LABEL: va6_no_fixed_args:
; VC16I:       ; %bb.0:
; VC16I-NEXT:    addi sp, -8
; VC16I-NEXT:    sw s1, 6(sp)
; VC16I-NEXT:    sw ra, 4(sp)
; VC16I-NEXT:    mv s1, sp
; VC16I-NEXT:    addi s1, 8
; VC16I-NEXT:    lli a0, 2
; VC16I-NEXT:    add a0, sp
; VC16I-NEXT:    sw a0, 0(sp)
; VC16I-NEXT:    mv sp, s1
; VC16I-NEXT:    addi sp, -8
; VC16I-NEXT:    lw ra, 4(sp)
; VC16I-NEXT:    lw s1, 6(sp)
; VC16I-NEXT:    addi sp, 8
; VC16I-NEXT:    jalr t0, ra, 0
  %va = alloca i8*, align 4
  %1 = bitcast i8** %va to i8*
  call void @llvm.va_start(i8* %1)
  %2 = va_arg i8** %va, i16
  call void @llvm.va_end(i8* %1)
  ret i16 %2
}
